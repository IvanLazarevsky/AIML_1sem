{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import gensim.parsing.preprocessing as gp\n",
    "from sklearn import feature_extraction, metrics\n",
    "from sklearn import naive_bayes, linear_model, svm\n",
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = train_data.data[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lerxst wam umd edu thing subject car nntp post host rac wam umd edu organ univers maryland colleg park line wonder enlighten car saw dai door sport car look late earli call bricklin door small addit bumper separ rest bodi know tellm model engin spec year product car histori info funki look car mail thank brought neighborhood lerxst\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokens_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_preprocessing(documents):\n",
    "    filters = gp.DEFAULT_FILTERS\n",
    "    return [gp.preprocess_string(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = gensim_preprocessing(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test = gensim_preprocessing(test_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer(preprocessor=lambda x:x,\n",
    "                                                           tokenizer=lambda x:x,max_features=30000)\n",
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "binarizer = Binarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = count_vectorizer.fit_transform(tokens_train)\n",
    "#X_train = binarizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vectorizer.transform(tokens_test)\n",
    "#X_test = binarizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighborhood 1\n",
      "brought 1\n",
      "thank 1\n",
      "mail 1\n",
      "funki 1\n",
      "info 1\n",
      "histori 1\n",
      "product 1\n",
      "year 1\n",
      "spec 1\n",
      "engin 1\n",
      "model 1\n",
      "tellm 1\n",
      "know 1\n",
      "bodi 1\n",
      "rest 1\n",
      "separ 1\n",
      "bumper 1\n",
      "addit 1\n",
      "small 1\n",
      "bricklin 1\n",
      "call 1\n",
      "earli 1\n",
      "late 1\n",
      "look 2\n",
      "sport 1\n",
      "door 2\n",
      "dai 1\n",
      "saw 1\n",
      "enlighten 1\n",
      "wonder 1\n",
      "line 1\n",
      "park 1\n",
      "colleg 1\n",
      "maryland 1\n",
      "univers 1\n",
      "organ 1\n",
      "rac 1\n",
      "host 1\n",
      "post 1\n",
      "nntp 1\n",
      "car 5\n",
      "subject 1\n",
      "thing 1\n",
      "edu 2\n",
      "umd 2\n",
      "wam 2\n",
      "lerxst 2\n"
     ]
    }
   ],
   "source": [
    "for i in X_train[0].indices:\n",
    "    print(feature_names[i], X_train[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinb:  0.8215613382899628\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.76      0.80      0.78       319\n",
      "           comp.graphics       0.68      0.79      0.73       389\n",
      " comp.os.ms-windows.misc       0.87      0.51      0.65       394\n",
      "comp.sys.ibm.pc.hardware       0.62      0.75      0.68       392\n",
      "   comp.sys.mac.hardware       0.75      0.83      0.79       385\n",
      "          comp.windows.x       0.84      0.78      0.81       395\n",
      "            misc.forsale       0.81      0.78      0.79       390\n",
      "               rec.autos       0.88      0.92      0.90       396\n",
      "         rec.motorcycles       0.93      0.96      0.94       398\n",
      "      rec.sport.baseball       0.95      0.94      0.95       397\n",
      "        rec.sport.hockey       0.95      0.97      0.96       399\n",
      "               sci.crypt       0.85      0.93      0.89       396\n",
      "         sci.electronics       0.78      0.72      0.75       393\n",
      "                 sci.med       0.92      0.85      0.88       396\n",
      "               sci.space       0.88      0.90      0.89       394\n",
      "  soc.religion.christian       0.85      0.92      0.89       398\n",
      "      talk.politics.guns       0.69      0.91      0.79       364\n",
      "   talk.politics.mideast       0.97      0.89      0.93       376\n",
      "      talk.politics.misc       0.73      0.59      0.65       310\n",
      "      talk.religion.misc       0.74      0.52      0.61       251\n",
      "\n",
      "               micro avg       0.82      0.82      0.82      7532\n",
      "               macro avg       0.82      0.81      0.81      7532\n",
      "            weighted avg       0.83      0.82      0.82      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "\n",
    "multi_nb.fit(X_train, train_data.target)\n",
    "m_pred = multi_nb.predict(X_test)\n",
    "print(\"Multinb: \", metrics.accuracy_score(test_data.target, m_pred))\n",
    "print(metrics.classification_report(test_data.target, m_pred, target_names = test_data.target_names))\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(test_data.target, m_pred)\n",
    "# for i, row in enumerate(confusion_matrix):\n",
    "#     print(test_data.target_names[i])\n",
    "#     for j, col in enumerate(row):\n",
    "#         print(test_data.target_names[j], \":\", col, end=' ')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.pyenv/versions/3.6.4/envs/general36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ivan/.pyenv/versions/3.6.4/envs/general36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_r = linear_model.LogisticRegression()\n",
    "log_r.fit(X_train, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_pred = log_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logr:  0.8007169410515136\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.76      0.70      0.72       319\n",
      "           comp.graphics       0.71      0.75      0.73       389\n",
      " comp.os.ms-windows.misc       0.73      0.68      0.71       394\n",
      "comp.sys.ibm.pc.hardware       0.64      0.72      0.68       392\n",
      "   comp.sys.mac.hardware       0.71      0.79      0.75       385\n",
      "          comp.windows.x       0.81      0.72      0.76       395\n",
      "            misc.forsale       0.77      0.86      0.81       390\n",
      "               rec.autos       0.87      0.85      0.86       396\n",
      "         rec.motorcycles       0.91      0.93      0.92       398\n",
      "      rec.sport.baseball       0.88      0.91      0.90       397\n",
      "        rec.sport.hockey       0.94      0.93      0.94       399\n",
      "               sci.crypt       0.93      0.90      0.91       396\n",
      "         sci.electronics       0.71      0.74      0.72       393\n",
      "                 sci.med       0.86      0.80      0.83       396\n",
      "               sci.space       0.91      0.90      0.90       394\n",
      "  soc.religion.christian       0.79      0.88      0.83       398\n",
      "      talk.politics.guns       0.73      0.87      0.79       364\n",
      "   talk.politics.mideast       0.96      0.81      0.88       376\n",
      "      talk.politics.misc       0.71      0.55      0.62       310\n",
      "      talk.religion.misc       0.64      0.59      0.62       251\n",
      "\n",
      "               micro avg       0.80      0.80      0.80      7532\n",
      "               macro avg       0.80      0.79      0.79      7532\n",
      "            weighted avg       0.80      0.80      0.80      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logr: \", metrics.accuracy_score(test_data.target, l_pred))\n",
    "print(metrics.classification_report(test_data.target, l_pred, target_names = test_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
