{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import gensim.parsing.preprocessing as gp\n",
    "from sklearn import feature_extraction, metrics\n",
    "from sklearn import naive_bayes, linear_model, svm\n",
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор данных 20 newsgroups состоит из множества usenet-постов из 20 тем. Задача заключается в опредении, к какой теме относится пост. Из постов удалены заголовки, подписи и цитаты (на семинаре мы этого не делали, поэтому сейчас результаты будут пореалистичнее). Набор данных встроен в sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\n",
    "test_data = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем пример текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "text = train_data.data[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так называются искомые темы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом туториале мы будем использовать bag-of-words представления и их производные. В базовом виде каждый текст $t$ представляется в виде вектора:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v}(t) = [c(w_1), c(w_2), ..., c(w_{|V|}]\n",
    "\\end{equation*}\n",
    "где $c(w_i)$ обозначает количество, сколько раз уникальное слово $w_i$ встретилось в тексте (счетчик слова $w_i$), а $|V|$ - общее количество уникальных слов (размер словаря). Словарь наполняется словами из всех текстов и опционально фильтруется. Bag-of-words векторы в подавляющем большинстве случаев разрежены - то есть, практически все их элементы равны нулю.\n",
    "\n",
    "Если для каждого слова использовать one-hot-кодировку, то bag-of-words - это векторная сумма последовательности кодировок слов из текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить BpW-представление, нужно извлечь собственно слова из текста, т.е. провести токенизацию текста. Вообще говоря, слово - условное понятие. Под ним можно подразумевать слова, знаки препинания, группы слов (например, все нецензурные слова можно считать вместе, а не по отдельности) и вообще произвольные счетные признаки.\n",
    "\n",
    "В обработке естественных языков преобработка (нормализация) текстов играет ключевую роль. Под ней подразумевается различная фильтрация лишних деталей, общие преобразования. Мы воспользуемся встроенной преобработкой из библиотеки gensim и оценим эффекты некоторых стадий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем текст в единый регистр, удалим html теги, пунктуацию, числа и стоп-слова (часто встречающиеся слова, которые без контекста практически не имеют смысла (пример https://gist.github.com/sebleier/554280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_preprocessing1(documents):\n",
    "    filters = [lambda s: s.lower(), gp.strip_tags, gp.strip_punctuation, gp.strip_numeric, gp.remove_stopwords]\n",
    "    return [gp.preprocess_string(doc,filters) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = gensim_preprocessing1(train_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем полученный список токенов из первого текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'enlighten', 'car', 'saw', 'day', 'door', 'sports', 'car', 'looked', 'late', 's', 'early', 's', 'called', 'bricklin', 'doors', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'engine', 'specs', 'years', 'production', 'car', 'history', 'info', 'funky', 'looking', 'car', 'e', 'mail']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test = gensim_preprocessing1(test_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь построим из списков токенов векторы bow. Для этого воспользуемся классом CountVectorizer из sklearn. Вообще, sklearn предоставляет свою ограниченную токенизацию и преобработку, но поскольку мы сделали её сами, заменим соответствующие шаги на ничего не делающие. Также для скорости ограничим словарь 30к самых частых слов (отметим, что мы выбросили стоп-слова)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer(preprocessor=lambda x:x,\n",
    "                                                           tokenizer=lambda x:x,max_features=30000)\n",
    "# tfidf = feature_extraction.text.TfidfTransformer()\n",
    "# binarizer = Binarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренируем векторизатор (т.е. дадим ему тексты, из которых он выяснит 30к самых частых слов и назначит им номера) и преобразуем тренировочные данные в векторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = count_vectorizer.fit_transform(tokens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vectorizer.transform(tokens_test)\n",
    "#X_test = binarizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 30000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем вектор первого текста. Поскольку в нем 30000 элементов, хранить их все было бы очень затратно. Поэтому хранятся в памяти только ненулевые элементы и их номера. Все векторы вместе образуют sparse-матрицу (X_train и X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15657)\t1\n",
      "  (0, 7962)\t1\n",
      "  (0, 15267)\t1\n",
      "  (0, 10416)\t1\n",
      "  (0, 12800)\t1\n",
      "  (0, 11872)\t1\n",
      "  (0, 20502)\t1\n",
      "  (0, 29710)\t1\n",
      "  (0, 24679)\t1\n",
      "  (0, 8457)\t1\n",
      "  (0, 16756)\t1\n",
      "  (0, 14116)\t1\n",
      "  (0, 3016)\t1\n",
      "  (0, 22156)\t1\n",
      "  (0, 23524)\t1\n",
      "  (0, 3463)\t1\n",
      "  (0, 287)\t1\n",
      "  (0, 24329)\t1\n",
      "  (0, 7579)\t1\n",
      "  (0, 3666)\t1\n",
      "  (0, 7975)\t1\n",
      "  (0, 22818)\t2\n",
      "  (0, 14502)\t1\n",
      "  (0, 15265)\t1\n",
      "  (0, 24800)\t1\n",
      "  (0, 7578)\t1\n",
      "  (0, 6339)\t1\n",
      "  (0, 23019)\t1\n",
      "  (0, 3786)\t4\n",
      "  (0, 8483)\t1\n",
      "  (0, 29002)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя хранимое в CountVectorizer отображение номеров на слова (feature_names), выведем счетчики слов первого текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mail 1\n",
      "e 1\n",
      "looking 1\n",
      "funky 1\n",
      "info 1\n",
      "history 1\n",
      "production 1\n",
      "years 1\n",
      "specs 1\n",
      "engine 1\n",
      "model 1\n",
      "know 1\n",
      "body 1\n",
      "rest 1\n",
      "separate 1\n",
      "bumper 1\n",
      "addition 1\n",
      "small 1\n",
      "doors 1\n",
      "called 1\n",
      "early 1\n",
      "s 2\n",
      "late 1\n",
      "looked 1\n",
      "sports 1\n",
      "door 1\n",
      "day 1\n",
      "saw 1\n",
      "car 4\n",
      "enlighten 1\n",
      "wondering 1\n"
     ]
    }
   ],
   "source": [
    "for i in X_train[0].indices:\n",
    "    print(feature_names[i], X_train[0,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренируем на полученных векторах наивный Байесовский классификатор. \n",
    "Используемая реализация NB использует следующую формулу для определения класса:\n",
    "\\begin{equation*}\n",
    "P(C|w_1, ..., w_{|V|}) = Z P(C)P(w_1, ..., w_{|V|}|C) = Z P(C)\\prod_{i = 1,~C(w_i) \\ne 0}^{|V|} P(w_i|C)\n",
    "\\end{equation*}\n",
    "Z - нормализующая константа, чтобы вероятности классов суммировались в 1.\n",
    "$P(w_i|C)$ - вероятность появления слова $w_i$ в тексте этого класса, она оценивается как:\n",
    "\\begin{equation*}\n",
    "P(w_i|C) = \\frac{\\sum{c(w_i) + \\alpha}}{\\sum_j [\\sum{c(w_j) + \\alpha}]}\n",
    "\\end{equation*}\n",
    "где $\\sum{c(w_i)}$ общее количество раз, которое данное слово встретилось во всех текстах класса $C$, $\\alpha$ - сглаживающая константа, благодаря которой у нас нет нулевых вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinb:  0.6512214551248009\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.53      0.48      0.50       319\n",
      "           comp.graphics       0.56      0.72      0.63       389\n",
      " comp.os.ms-windows.misc       0.18      0.01      0.01       394\n",
      "comp.sys.ibm.pc.hardware       0.48      0.73      0.58       392\n",
      "   comp.sys.mac.hardware       0.64      0.62      0.63       385\n",
      "          comp.windows.x       0.68      0.78      0.72       395\n",
      "            misc.forsale       0.82      0.70      0.75       390\n",
      "               rec.autos       0.78      0.71      0.74       396\n",
      "         rec.motorcycles       0.83      0.70      0.76       398\n",
      "      rec.sport.baseball       0.90      0.81      0.85       397\n",
      "        rec.sport.hockey       0.58      0.89      0.71       399\n",
      "               sci.crypt       0.70      0.74      0.72       396\n",
      "         sci.electronics       0.65      0.52      0.58       393\n",
      "                 sci.med       0.82      0.77      0.79       396\n",
      "               sci.space       0.77      0.73      0.75       394\n",
      "  soc.religion.christian       0.53      0.85      0.66       398\n",
      "      talk.politics.guns       0.56      0.68      0.61       364\n",
      "   talk.politics.mideast       0.76      0.75      0.76       376\n",
      "      talk.politics.misc       0.46      0.44      0.45       310\n",
      "      talk.religion.misc       0.40      0.13      0.20       251\n",
      "\n",
      "               micro avg       0.65      0.65      0.65      7532\n",
      "               macro avg       0.63      0.64      0.62      7532\n",
      "            weighted avg       0.64      0.65      0.63      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "\n",
    "multi_nb.fit(X_train, train_data.target)\n",
    "m_pred = multi_nb.predict(X_test)\n",
    "print(\"Multinb: \", metrics.accuracy_score(test_data.target, m_pred))\n",
    "print(metrics.classification_report(test_data.target, m_pred, target_names = test_data.target_names))\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(test_data.target, m_pred)\n",
    "# for i, row in enumerate(confusion_matrix):\n",
    "#     print(test_data.target_names[i])\n",
    "#     for j, col in enumerate(row):\n",
    "#         print(test_data.target_names[j], \":\", col, end=' ')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты не очень, попробуем добавить стеммизацию (грубую обработку слов по морфологическим правилам, которая сводит большинство форм одного слова в одну), а также удалить все короткие слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_preprocessing2(documents):\n",
    "    filters = [lambda s: s.lower(), gp.strip_tags, gp.strip_punctuation, \n",
    "               gp.strip_numeric, gp.remove_stopwords, gp.strip_short, gp.stem_text]\n",
    "    return [gp.preprocess_string(doc,filters) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stem_train = gensim_preprocessing2(train_data.data)\n",
    "tokens_stem_test = gensim_preprocessing2(test_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonder', 'enlighten', 'car', 'saw', 'dai', 'door', 'sport', 'car', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'car', 'histori', 'info', 'funki', 'look', 'car', 'mail']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_stem_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_stem = feature_extraction.text.CountVectorizer(preprocessor=lambda x:x,\n",
    "                                                           tokenizer=lambda x:x,max_features=30000)\n",
    "X_train_stem = cv_stem.fit_transform(tokens_stem_train)\n",
    "X_test_stem = cv_stem.transform(tokens_stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinb:  0.6533457249070632\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.54      0.45      0.49       319\n",
      "           comp.graphics       0.60      0.71      0.65       389\n",
      " comp.os.ms-windows.misc       0.81      0.20      0.32       394\n",
      "comp.sys.ibm.pc.hardware       0.52      0.67      0.59       392\n",
      "   comp.sys.mac.hardware       0.65      0.61      0.63       385\n",
      "          comp.windows.x       0.66      0.77      0.71       395\n",
      "            misc.forsale       0.82      0.66      0.73       390\n",
      "               rec.autos       0.77      0.73      0.75       396\n",
      "         rec.motorcycles       0.84      0.69      0.76       398\n",
      "      rec.sport.baseball       0.91      0.77      0.83       397\n",
      "        rec.sport.hockey       0.58      0.90      0.70       399\n",
      "               sci.crypt       0.63      0.77      0.69       396\n",
      "         sci.electronics       0.63      0.49      0.55       393\n",
      "                 sci.med       0.81      0.79      0.80       396\n",
      "               sci.space       0.76      0.74      0.75       394\n",
      "  soc.religion.christian       0.52      0.85      0.65       398\n",
      "      talk.politics.guns       0.55      0.68      0.61       364\n",
      "   talk.politics.mideast       0.74      0.76      0.75       376\n",
      "      talk.politics.misc       0.45      0.43      0.44       310\n",
      "      talk.religion.misc       0.40      0.12      0.19       251\n",
      "\n",
      "               micro avg       0.65      0.65      0.65      7532\n",
      "               macro avg       0.66      0.64      0.63      7532\n",
      "            weighted avg       0.67      0.65      0.64      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_nb_stem = naive_bayes.MultinomialNB()\n",
    "\n",
    "multi_nb_stem.fit(X_train_stem, train_data.target)\n",
    "m_pred = multi_nb_stem.predict(X_test_stem)\n",
    "print(\"Multinb: \", metrics.accuracy_score(test_data.target, m_pred))\n",
    "print(metrics.classification_report(test_data.target, m_pred, target_names = test_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат практически не изменился. Добавим ещё 30000 атрибутов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinb:  0.6427243759957515\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.60      0.38      0.46       319\n",
      "           comp.graphics       0.61      0.71      0.66       389\n",
      " comp.os.ms-windows.misc       0.89      0.14      0.25       394\n",
      "comp.sys.ibm.pc.hardware       0.53      0.66      0.59       392\n",
      "   comp.sys.mac.hardware       0.70      0.57      0.63       385\n",
      "          comp.windows.x       0.60      0.79      0.68       395\n",
      "            misc.forsale       0.84      0.62      0.71       390\n",
      "               rec.autos       0.80      0.72      0.76       396\n",
      "         rec.motorcycles       0.88      0.66      0.75       398\n",
      "      rec.sport.baseball       0.93      0.73      0.82       397\n",
      "        rec.sport.hockey       0.57      0.91      0.71       399\n",
      "               sci.crypt       0.55      0.80      0.65       396\n",
      "         sci.electronics       0.66      0.50      0.57       393\n",
      "                 sci.med       0.79      0.79      0.79       396\n",
      "               sci.space       0.75      0.75      0.75       394\n",
      "  soc.religion.christian       0.49      0.88      0.63       398\n",
      "      talk.politics.guns       0.55      0.64      0.59       364\n",
      "   talk.politics.mideast       0.65      0.78      0.71       376\n",
      "      talk.politics.misc       0.45      0.44      0.44       310\n",
      "      talk.religion.misc       0.45      0.07      0.12       251\n",
      "\n",
      "               micro avg       0.64      0.64      0.64      7532\n",
      "               macro avg       0.66      0.63      0.61      7532\n",
      "            weighted avg       0.67      0.64      0.63      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_stem2 = feature_extraction.text.CountVectorizer(preprocessor=lambda x:x,\n",
    "                                                           tokenizer=lambda x:x,max_features=60000)\n",
    "X_train_stem2 = cv_stem2.fit_transform(tokens_stem_train)\n",
    "X_test_stem2 = cv_stem2.transform(tokens_stem_test)\n",
    "\n",
    "multi_nb_stem2 = naive_bayes.MultinomialNB()\n",
    "\n",
    "multi_nb_stem2.fit(X_train_stem2, train_data.target)\n",
    "m_pred = multi_nb_stem2.predict(X_test_stem2)\n",
    "print(\"Multinb: \", metrics.accuracy_score(test_data.target, m_pred))\n",
    "print(metrics.classification_report(test_data.target, m_pred, target_names = test_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взвесим слова при помощи idf-весов (обратная документная частота). Воспользуемся схемой $idf_i = log \\frac{N}{df_i}$, где $df_i$ - количество документов, в которых встретилось слово, а $N$ общее количество документов. Таким образом, слова более уникальные для документов имеют больший вес, общераспространенные - меньший. После это мы получаем tf-idf представление вида. Представление также нормализовано по длине, чтобы сгладить эффект более длинных текстов:\n",
    "\\begin{equation*}\n",
    "\\vec{v}(t) = [idf_1*c(w_1), idf_2*c(w_2), ..., idf_{|V|}*c(w_{|V|})]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinb:  0.6708709506107275\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.75      0.20      0.31       319\n",
      "           comp.graphics       0.68      0.69      0.68       389\n",
      " comp.os.ms-windows.misc       0.66      0.54      0.59       394\n",
      "comp.sys.ibm.pc.hardware       0.57      0.69      0.62       392\n",
      "   comp.sys.mac.hardware       0.72      0.66      0.69       385\n",
      "          comp.windows.x       0.78      0.76      0.77       395\n",
      "            misc.forsale       0.77      0.71      0.74       390\n",
      "               rec.autos       0.81      0.73      0.77       396\n",
      "         rec.motorcycles       0.83      0.72      0.77       398\n",
      "      rec.sport.baseball       0.93      0.79      0.85       397\n",
      "        rec.sport.hockey       0.57      0.93      0.71       399\n",
      "               sci.crypt       0.60      0.79      0.68       396\n",
      "         sci.electronics       0.70      0.51      0.59       393\n",
      "                 sci.med       0.83      0.77      0.80       396\n",
      "               sci.space       0.78      0.76      0.77       394\n",
      "  soc.religion.christian       0.39      0.92      0.54       398\n",
      "      talk.politics.guns       0.55      0.73      0.63       364\n",
      "   talk.politics.mideast       0.83      0.78      0.80       376\n",
      "      talk.politics.misc       0.85      0.31      0.45       310\n",
      "      talk.religion.misc       1.00      0.01      0.02       251\n",
      "\n",
      "               micro avg       0.67      0.67      0.67      7532\n",
      "               macro avg       0.73      0.65      0.64      7532\n",
      "            weighted avg       0.72      0.67      0.66      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "X_train_idf = tfidf.fit_transform(X_train_stem)\n",
    "X_test_idf = tfidf.transform(X_test_stem)\n",
    "\n",
    "nb3 = naive_bayes.MultinomialNB()\n",
    "nb3.fit(X_train_idf, train_data.target)\n",
    "nb3_pred = nb3.predict(X_test_idf)\n",
    "print(\"Multinb: \", metrics.accuracy_score(test_data.target, nb3_pred))\n",
    "print(metrics.classification_report(test_data.target, nb3_pred, target_names = test_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренируем также логистическую регрессию на этих же данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.pyenv/versions/3.6.4/envs/general36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ivan/.pyenv/versions/3.6.4/envs/general36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logr:  0.6808284652150823\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.51      0.47      0.49       319\n",
      "           comp.graphics       0.66      0.69      0.68       389\n",
      " comp.os.ms-windows.misc       0.65      0.59      0.62       394\n",
      "comp.sys.ibm.pc.hardware       0.64      0.62      0.63       392\n",
      "   comp.sys.mac.hardware       0.72      0.67      0.69       385\n",
      "          comp.windows.x       0.80      0.70      0.75       395\n",
      "            misc.forsale       0.70      0.76      0.73       390\n",
      "               rec.autos       0.78      0.71      0.74       396\n",
      "         rec.motorcycles       0.49      0.78      0.60       398\n",
      "      rec.sport.baseball       0.79      0.80      0.80       397\n",
      "        rec.sport.hockey       0.89      0.88      0.89       399\n",
      "               sci.crypt       0.87      0.69      0.77       396\n",
      "         sci.electronics       0.54      0.60      0.57       393\n",
      "                 sci.med       0.75      0.80      0.77       396\n",
      "               sci.space       0.73      0.73      0.73       394\n",
      "  soc.religion.christian       0.62      0.79      0.70       398\n",
      "      talk.politics.guns       0.58      0.66      0.62       364\n",
      "   talk.politics.mideast       0.85      0.75      0.79       376\n",
      "      talk.politics.misc       0.57      0.45      0.50       310\n",
      "      talk.religion.misc       0.50      0.19      0.27       251\n",
      "\n",
      "               micro avg       0.68      0.68      0.68      7532\n",
      "               macro avg       0.68      0.67      0.67      7532\n",
      "            weighted avg       0.69      0.68      0.68      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logr = linear_model.LogisticRegression()\n",
    "logr.fit(X_train_idf, train_data.target)\n",
    "logr_pred = logr.predict(X_test_idf)\n",
    "print(\"Logr: \", metrics.accuracy_score(test_data.target, logr_pred))\n",
    "print(metrics.classification_report(test_data.target, logr_pred, target_names = test_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого класса также выведем наиболее важные в положительном и отрицательном смысле слова, с точки зрения логистической регресии. На каждый класс натренирована линейная функция и выбирается тот класс, для которого соответствующая функция вернула максимальное значение (см. softmax-регрессия)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 30000)\n"
     ]
    }
   ],
   "source": [
    "print(logr.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "Most important(+) post delet punish bobbi motto moral atheism religion atheist islam\n",
      "Most important(-) window thank us want game work mail christ drive christian\n",
      "comp.graphics\n",
      "Most important(+) format anim cview tiff program algorithm file polygon imag graphic\n",
      "Most important(-) peopl drive kei car window right god win monitor believ\n",
      "comp.os.ms-windows.misc\n",
      "Most important(+) risc problem microsoft font cica win max driver file window\n",
      "Most important(-) year car bit game time power server state peopl sale\n",
      "comp.sys.ibm.pc.hardware\n",
      "Most important(+) gatewai vlb motherboard irq id drive card bu monitor scsi\n",
      "Most important(-) mac peopl car appl year window case god includ offer\n",
      "comp.sys.mac.hardware\n",
      "Most important(+) lciii nubu problem duo powerbook simm quadra centri appl mac\n",
      "Most important(-) window do car control id god file game com year\n",
      "comp.windows.x\n",
      "Most important(+) displai applic sun client mit window xterm server motif widget\n",
      "Most important(-) do card drive mac god driver car game lot post\n",
      "misc.forsale\n",
      "Most important(+) price interest new email condit includ ship sell offer sale\n",
      "Most important(-) think know help read peopl problem file appreci team sure\n",
      "rec.autos\n",
      "Most important(+) wheel road toyota wagon engin dealer oil auto ford car\n",
      "Most important(-) bike game file card program god team christian plai softwar\n",
      "rec.motorcycles\n",
      "Most important(+) biker harlei dog rider bmw helmet motorcycl dod ride bike\n",
      "Most important(-) card window game us program kei believ team christian file\n",
      "rec.sport.baseball\n",
      "Most important(+) philli bat brave stadium cub pitcher hit year pitch basebal\n",
      "Most important(-) hockei window us car peopl file work nhl playoff problem\n",
      "rec.sport.hockey\n",
      "Most important(+) coach player season leaf nhl playoff plai game team hockei\n",
      "Most important(-) run us work window drive pitch file god problem program\n",
      "sci.crypt\n",
      "Most important(+) clinton privaci phone govern chip secur nsa clipper encrypt kei\n",
      "Most important(-) window drive god thank car problem card christian help game\n",
      "sci.electronics\n",
      "Most important(+) scope detector wire signal ground amp power voltag electron circuit\n",
      "Most important(-) window file peopl mac bike think govern year god encrypt\n",
      "sci.med\n",
      "Most important(+) cancer effect pain treatment food patient medic diseas msg doctor\n",
      "Most important(-) god window car game drive christian file govern card space\n",
      "sci.space\n",
      "Most important(+) satellit earth rocket shuttl spacecraft moon nasa launch orbit space\n",
      "Most important(-) window car drive game god card chip kei run bike\n",
      "soc.religion.christian\n",
      "Most important(+) resurrect jesu marriag faith cathol christ sin god christian church\n",
      "Most important(-) window game car run file us drive right atheism team\n",
      "talk.politics.guns\n",
      "Most important(+) law handheld crimin nra jmd batf fbi firearm weapon gun\n",
      "Most important(-) god window game armenian kei problem program christian encrypt israel\n",
      "talk.politics.mideast\n",
      "Most important(+) kill turkei greek palestinian turkish jew arab armenian isra israel\n",
      "Most important(-) look us thank window game god jesu work gun drive\n",
      "talk.politics.misc\n",
      "Most important(+) gai state trial govern clinton homosexu drug presid libertarian tax\n",
      "Most important(-) god christian gun window chip thank kei team game armenian\n",
      "talk.religion.misc\n",
      "Most important(+) tyre mormon god rosicrucian object jesu moral kent koresh christian\n",
      "Most important(-) thank need edu window problem drive file car work game\n"
     ]
    }
   ],
   "source": [
    "words = cv_stem.get_feature_names()\n",
    "for i in range(20):\n",
    "    indices = logr.coef_[i].argsort()\n",
    "    least = indices[:10]\n",
    "    most = indices[-10:]\n",
    "    \n",
    "    print(train_data.target_names[i])\n",
    "    print('Most important(+)', ' '.join([words[j] for j in most]))\n",
    "    print('Most important(-)', ' '.join([words[j] for j in least]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
