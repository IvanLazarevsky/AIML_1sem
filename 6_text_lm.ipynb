{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная языковая модель. Векторы слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import gensim.parsing.preprocessing as gp\n",
    "import nltk\n",
    "from sklearn import feature_extraction, metrics\n",
    "from sklearn import naive_bayes, linear_model, svm\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from keras import models, layers, utils, callbacks, optimizers\n",
    "from itertools import chain\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковая модель задает распределение вероятности над строками языка. Если строки состоят из слов (есть также посимвольные модели), то по сути модель задаёт распределение вероятностей вида $P(w_1,w_2,...,w_n)$, где $n$ - длина строки. Типичная модель описывает распределение $P(w_i~|~w_1,...,w_{i-1})$, тогда $P(w_1,w_2,...,w_n) = P(w_n~|~w_1,w_2,...,w_{n-1})P(w_1,w_2,...,w_{n-1})$.\n",
    "\n",
    "Марковские модели $k$-го порядка упрощают распределение $P(w_i~|~w_1,...,w_{i-1})$ до $P(w_i~|~w_{i-k-1},...,w_{i-1})$, т.е. вероятность следующего слова зависит только от $k+1$ предыдущих слов. Они также называются n-gram моделями. Модель нулевого порядка называется униграм моделью, первого - биграм моделью, второго, третьего и четвертого - триграм-моделью, 4-грам, 5-грам и т.д.\n",
    "\n",
    "Языковые модели активно применяются в задачах интерпретации, например, при распознавании речи. Аналогично можно искать наиболее вероятное исправление текста, наиболее вероятный перевод фразы. Мы часто сталкиваемся с языковыми моделями когда набираем текст на телефоне.\n",
    "<img src=\"predictive_keyboard1.png\">\n",
    "Разумеется, сэмплирование из языковой модели позволяет генерировать текст. В зависимости от типа модели и данных, на которых она натренирована, данный текст будет в большей или меньшей степени похож на \"настоящий\".\n",
    "В этой тетради мы построим модель, натренированную на форумных сообщениях из набора данных 20 newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fetch_20newsgroups(subset='train',remove=['headers', 'footers', 'quotes'])\n",
    "test_data = fetch_20newsgroups(subset='test',remove=['headers', 'footers', 'quotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "text = train_data.data[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку наша задача - генерировать более-менее натуральный текст, мы воспользуемся куда более аккуратной токенизацией из библиотеки NLTK, по сравнению с грубой обработкой в прошлом туториале. Единственное, текст все равно переведен в нижний регистр и соотв. генерироваться будет аналогично, это сделано, чтобы немного уменьшить объём данных для тренировки и объём словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized(documents):\n",
    "    def process_document(doc: str):\n",
    "        words = nltk.tokenize.word_tokenize(doc)\n",
    "        return [w.lower() for w in words]\n",
    "    return  [process_document(doc) for doc in documents] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenized(train_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы используем и тренировочные и тестовые данные из задачи классификации для тренировки модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train.extend(tokenized(test_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', '.', 'it', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'it', 'was', 'called', 'a', 'bricklin', '.', 'the', 'doors', 'were', 'really', 'small', '.', 'in', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'this', 'is', 'all', 'i', 'know', '.', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Назначим каждому слову номер, используя CountVectorizer. Поскольку эта тетрадь запускалась несколько раз с целью дотренировки модели, было решено сохранить полученный словарь в файл и загружать его, поскольку я не уверен в детерминированности CountVectorizer. Поскольку тренируется 4-грам модель $P(w_4|w_1,w_2,w_3)$, введены специальные символы для начала текста `<S>`, конца `</S>`. Поскольку словарь модели будет ограничен, все слова встречающиеся менее 40000 раз будут заменены на спец. слово `<UNK>`. Вообще говоря, это спец. слово можно разнообразить, например, ввести слова `<UNK_NOUN>`, `<UNK_VERB>`, соотв. частям речи, но здесь мы этого не делаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vectorizer = feature_extraction.text.CountVectorizer(preprocessor=lambda x:x,\n",
    "#                                                            tokenizer=lambda x:x, max_features=40000)\n",
    "# count_vectorizer.fit(tokens_train)\n",
    "# feature_names = count_vectorizer.get_feature_names()\n",
    "# feature_names.append('<UNK>')\n",
    "# feature_names.append('<S>')\n",
    "# feature_names.append('</S>')\n",
    "\n",
    "# with open('nltk_feature_names4_wm.json','w+') as of:\n",
    "#     json.dump(feature_names,of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем файл со словарем (списком слов). Используя этот список назначим каждому слову номер (индекс в списке). Преобразуем все тексты (списки слов) в списки номеров этих слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nltk_feature_names4_wm.json') as f:\n",
    "    feature_names = json.load(f)\n",
    "vocab = {v:i for i,v in enumerate(feature_names)}\n",
    "unk_index = vocab['<UNK>']\n",
    "n_words = len(feature_names)\n",
    "end_index = vocab['</S>']\n",
    "start_index = vocab['<S>']\n",
    "\n",
    "ids_train = []\n",
    "ids_test = []\n",
    "for row in tokens_train:\n",
    "    ids_train.append([vocab.get(word, unk_index) for word in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные тексты в виде списков номеров разобъём на четвёрки (4-грамы) и поместим их в единый массив, выведем первые 10 элементов. Всего в массиве 4760679 n-gramов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40001, 40001, 40001, 20442), (40001, 40001, 20442, 38521), (40001, 20442, 38521, 38996), (20442, 38521, 38996, 20582), (38521, 38996, 20582, 7435), (38996, 20582, 7435, 26981), (20582, 7435, 26981, 36183), (7435, 26981, 36183, 13070), (26981, 36183, 13070, 16741), (36183, 13070, 16741, 24293)]\n",
      "4760679\n"
     ]
    }
   ],
   "source": [
    "ngrams = []\n",
    "for row in ids_train:\n",
    "    for ngram in nltk.ngrams(row,4,pad_left=True, pad_right=True,\n",
    "                               left_pad_symbol=start_index, right_pad_symbol=end_index):\n",
    "        ngrams.append(ngram)\n",
    "        \n",
    "print(ngrams[:10])\n",
    "print(len(ngrams))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем в numpy формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4760679, 4)\n"
     ]
    }
   ],
   "source": [
    "ngrams = np.array(ngrams)\n",
    "print(ngrams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве модели распределения $P(w_4|w_1,w_2,w_3)$ будем использовать нейронную сеть. \n",
    "Есть более простые, собственно n-gram модели, которые оценивают эти вероятности через относительные частоты и сглаживание (аналогично описанному в прежней тетради Наивному Байесовскому классификатору). Они тренируются очень быстро, но при этом потребляют много памяти. Выбор нейронной сети обусловлен некоторыми интересными свойствами получаемых моделей, а также их более высоким в целом качеством. Качество модели замеряется как правило через перплексию и кросс-энтропию - среднюю неопределенность следующего слова при известных предыдущих (меньше - лучше).\n",
    "\n",
    "Модели на вход поставляются 3 слова в виде one-hot векторов, т.е. 40000-мерных векторов, в которых все элементы, кроме одного равного единице, равны нулю. Для каждого из трех слов используется одна и та же матрица весов, которая умножается на этот вектор, результат равен одному из её столбцов. Этот столбец называется <b>вектором слова</b>, а само преобразование категориальной переменной в вектор - <b>встраиванием (embedding</b>). Матрица весов соотв. называется встраивающей матрицей (Embedding матрицей). Полученные три вектора конкатенируются и подаются на скрытые слои сети, которые производят различные преобразования над ними. Последний слой использует функцию softmax для создания дискретного распределения вероятности над 40000 словами. \n",
    "\n",
    "<img src=\"langnet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сети нужны one-hot векторы в качестве эталонных выходов, и преобразовать все выходы в них сразу было бы убийственно по памяти, поэтому генерироваться экземпляры для тренировки будут налету. Сеть будет тренироваться пачками экземпляров фиксированного размера и перед каждой итерацией эти пачки будут генерироваться нижеописанной функцией. Три номера с каждого экземпляра попадают в матрицу $X$, а последний номер преобразовывается в one-hot вектор и попадает в матрицу $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size=128):\n",
    "    while True:\n",
    "        indices = np.random.randint(0, len(ngrams),size = batch_size)\n",
    "        rows = ngrams[indices]\n",
    "        X = rows[:,:-1]\n",
    "        labels = rows[:,-1]\n",
    "        y = utils.to_categorical(labels, num_classes=n_words)\n",
    "        yield X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 3) int64\n",
      "(128, 40003)\n"
     ]
    }
   ],
   "source": [
    "XX,yy = next(make_batch())\n",
    "print(XX.shape, XX.dtype)\n",
    "print(yy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим сеть. Её первый слой задает для каждого слова обучаемый вектор размером 130. Далее эти векторы конкатенируются и подаются на следующий слой из 400 элементов, с кусочно-линейной активацией. Далее есть спец.слой нормализации, который проводит простое преобразование данных так, чтобы их среднее было близко к 0, а стандартное отклонение к 1. Нормализация данных часто ускоряет обучение сети, хотя я уже не помню, помогла ли она здесь. В любом случае этот слой не навредил. Последний слой имеет размерность 40003 и на его выходе распределение вероятностей (т.е. его выход суммируется в 1). softmax активация дает модели гибкость в приближении выхода по форме к one-hot вектору (см. введение в sklearn про softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(input_dim=n_words,output_dim=130, input_length=3))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(400))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(units=n_words, activation='softmax'))\n",
    "\n",
    "optimizer = optimizers.Adagrad()\n",
    "model.compile(optimizer,loss='categorical_crossentropy')\n",
    "\n",
    "# model = models.load_model('language_model_nltk42best_wm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Долго и мучительно тренируем модель. Хотя в данном случае использование оптимизатора Adagrad заметно улучшило скорость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 239s 119ms/step - loss: 5.8739 - val_loss: 5.4960\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 222s 111ms/step - loss: 5.4761 - val_loss: 5.3675\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 222s 111ms/step - loss: 5.3233 - val_loss: 5.3186\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 222s 111ms/step - loss: 5.2353 - val_loss: 5.1612\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 223s 111ms/step - loss: 5.1647 - val_loss: 5.1096\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 223s 112ms/step - loss: 5.1029 - val_loss: 5.0213\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 5.0634 - val_loss: 5.0833\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 5.0199 - val_loss: 5.0254\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.9786 - val_loss: 4.9600\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.9551 - val_loss: 4.9790\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.9090 - val_loss: 4.8264\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.9122 - val_loss: 4.8699\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.8560 - val_loss: 4.6862\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.8329 - val_loss: 4.8340\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.8084 - val_loss: 4.7514\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.7890 - val_loss: 4.7303\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.7662 - val_loss: 4.7798\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.7607 - val_loss: 4.6492\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.7393 - val_loss: 4.7335\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.7209 - val_loss: 4.6822\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.6981 - val_loss: 4.6139\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.6816 - val_loss: 4.7003\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.6666 - val_loss: 4.6773\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.6517 - val_loss: 4.6494\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.6395 - val_loss: 4.6543\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.6253 - val_loss: 4.5916\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.6153 - val_loss: 4.5605\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.5867 - val_loss: 4.5463\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.5656 - val_loss: 4.5660\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.5732 - val_loss: 4.5540\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.5564 - val_loss: 4.5457\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.5416 - val_loss: 4.4679\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.5356 - val_loss: 4.5567\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.5188 - val_loss: 4.5714\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.5177 - val_loss: 4.5455\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.5085 - val_loss: 4.4175\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.5038 - val_loss: 4.4201\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4900 - val_loss: 4.4974\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4769 - val_loss: 4.4232\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4453 - val_loss: 4.3921\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4637 - val_loss: 4.4366\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4402 - val_loss: 4.4731\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4391 - val_loss: 4.4755\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4346 - val_loss: 4.4336\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4083 - val_loss: 4.3839\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.4065 - val_loss: 4.4527\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.3924 - val_loss: 4.4027\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.3887 - val_loss: 4.2692\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.3753 - val_loss: 4.3876\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 224s 112ms/step - loss: 4.3814 - val_loss: 4.4006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c0145cc88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(make_batch(128),steps_per_epoch=2000,epochs=50, validation_data=make_batch(128),\n",
    "#                     validation_steps=40,\n",
    "#                     callbacks=[ callbacks.ModelCheckpoint('language_model_nltk42best_wm.h5',save_best_only=True),\n",
    "#                                 callbacks.ModelCheckpoint('language_model_nltk42latest_wm.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 225s 112ms/step - loss: 4.1061 - val_loss: 4.0952\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 225s 113ms/step - loss: 4.1083 - val_loss: 4.0874\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.1060 - val_loss: 4.0673\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0953 - val_loss: 4.0958\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0810 - val_loss: 4.0461\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0930 - val_loss: 4.0500\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 229s 114ms/step - loss: 4.0822 - val_loss: 4.0516\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 228s 114ms/step - loss: 4.0854 - val_loss: 4.0773\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 227s 114ms/step - loss: 4.0788 - val_loss: 4.0575\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0709 - val_loss: 4.0657\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0803 - val_loss: 4.0769\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0787 - val_loss: 4.0174\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0587 - val_loss: 4.0720\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0578 - val_loss: 4.0533\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0455 - val_loss: 4.0114\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 4.0457 - val_loss: 4.0705\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0519 - val_loss: 4.0280\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0485 - val_loss: 4.0194\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0411 - val_loss: 4.0257\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0438 - val_loss: 4.0436\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0393 - val_loss: 3.9799\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0274 - val_loss: 4.0101\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0159 - val_loss: 3.9981\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0338 - val_loss: 4.0164\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0287 - val_loss: 3.9516\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0268 - val_loss: 4.0079\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0138 - val_loss: 3.9941\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0113 - val_loss: 4.0059\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0046 - val_loss: 3.9783\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0022 - val_loss: 4.0050\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0089 - val_loss: 3.9646\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0044 - val_loss: 3.9887\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9982 - val_loss: 3.9883\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 4.0012 - val_loss: 3.9500\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9909 - val_loss: 3.9339\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9868 - val_loss: 3.9960\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9827 - val_loss: 3.9917\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9853 - val_loss: 3.9777\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9779 - val_loss: 3.9893\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9789 - val_loss: 3.9803\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 226s 113ms/step - loss: 3.9825 - val_loss: 3.9642\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 3.9764 - val_loss: 3.9475\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 227s 114ms/step - loss: 3.9814 - val_loss: 3.9580\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 3.9615 - val_loss: 3.9222\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 227s 114ms/step - loss: 3.9654 - val_loss: 3.9844\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 3.9622 - val_loss: 3.9091\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 3.9709 - val_loss: 3.9703\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 3.9456 - val_loss: 3.9548\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 3.9592 - val_loss: 3.9600\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 227s 113ms/step - loss: 3.9459 - val_loss: 3.9415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c009e3080>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(make_batch(128),steps_per_epoch=2000,epochs=50, validation_data=make_batch(128),\n",
    "#                     validation_steps=100,\n",
    "#                     callbacks=[ callbacks.ModelCheckpoint('language_model_nltk42best_wm_cont2.h5',save_best_only=True),\n",
    "#                                 callbacks.ModelCheckpoint('language_model_nltk42latest_wm_cont2.h5')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После каждой крупной итерации модель сохранялась и поскольку она уже натренирована, просто загрузим её из файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('language_model_nltk42latest_wm_cont2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция возвращает в порядке убывания N наиболее вероятных слов и их вероятность, при заданных 3х предыдущих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distr(nn, start_vector, N):\n",
    "    start_vector = np.asarray(start_vector)\n",
    "    pred = nn.predict(start_vector.reshape(1,-1)).ravel()\n",
    "    ml = np.argsort(pred)[::-1]\n",
    "    return [ (index, pred[index]) for i, index in zip(range(N), ml)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.21830438\n",
      "<UNK> 0.0982188\n",
      "a 0.023411596\n",
      "washington 0.010317443\n",
      "another 0.009617392\n",
      "pluto 0.009405057\n",
      "this 0.007506593\n",
      "jerusalem 0.0071299984\n",
      "nasa 0.0069796834\n",
      "school 0.006576244\n"
     ]
    }
   ],
   "source": [
    "for index, prob in distr(model, [vocab['to'], vocab['travel'], vocab['to']], 10):\n",
    "    print(feature_names[index], prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию сэмплирования из модели. Ей на вход подается три предыдущих слова, а генерирует она текст длиной k или пока не будет сгенерирован символ конца текста. Для этого используется взвешенный случайный выбор из 40003 слов. По-умолчанию также исключается генерация символа `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(nn, k, length, seed_vector, generate_unk=False):\n",
    "    results = []\n",
    "    indices = np.arange(len(feature_names))\n",
    "    for i in range(k):\n",
    "        start_vector = np.array(seed_vector)\n",
    "        res = [feature_names[ind] for ind in start_vector]\n",
    "        for _ in range(length):\n",
    "            weights = nn.predict(start_vector.reshape(1,-1)).ravel()\n",
    "            if not generate_unk:\n",
    "                weights[unk_index] = 0\n",
    "                weights /= weights.sum()\n",
    "            next_ind = np.random.choice(indices,p=weights)\n",
    "            start_vector[0], start_vector[1], start_vector[2] = start_vector[1], start_vector[2], next_ind\n",
    "            if next_ind == end_index:\n",
    "                break\n",
    "            res.append(feature_names[next_ind])\n",
    "        results.append(res)\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to travel to this arena every rumor ! . i think the attitude\n",
      "to travel to orbit between the network size price for use windows .\n",
      "to travel to pluto a widget creation will one only person sometime off\n",
      "to travel to light with heaps of stone and dust and when i\n",
      "to travel to his return . this is a good choice . i\n",
      "to travel to any match law , is termed nearly optimum on secrecy\n",
      "to travel to the kinsey 's gift and public domain without peer to\n",
      "to travel to 6-0 a legal place to call for the next day\n",
      "to travel to the combined with each displays of multiple parents . has\n",
      "to travel to armenia , the 22.9 ( 1-1 ) 18 tor mark\n",
      "to travel to lawful lynn mitre corporation . sci.electronics '' _* # 3\n",
      "to travel to this morning , the vast majority are there any summaries\n",
      "to travel to developing iraq ( specifically , running yet before what happens\n",
      "to travel to blue may cost ) $ 3.00 wolverine 1 ( 1982\n",
      "to travel to heaven ? and building 6 applications . these are you\n",
      "to travel to the account of the bible hype about this `` post\n",
      "to travel to telephone and security . i 'm going to do business\n",
      "to travel to 4gb of xt specification is typically defined a j14 rsh\n",
      "to travel to path formats of any kind of opinion on particular group\n",
      "to travel to biological ... he is not hoped that was another woman\n"
     ]
    }
   ],
   "source": [
    "samples = sample_from_model(model, 20, 10, [vocab['to'], vocab['travel'], vocab['to']])\n",
    "for s in samples:\n",
    "    print(' '.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my suggestion i have some kind ( environmental disaster ? senior , government engineering research research secretary of the caucasus . so , in your generation may play less than the best at one point it is actually grown up . -- have you already asked . -- -- -- -- -- -- file : 1 ) what about it ? as a result of n't you had a lot of `` sucking '' the literature of research demand the town of khojaly and a stark of secrecy and probably x $ 25 pad ( increased eisa contract without higher\n",
      "---------\n",
      "i have number were hard to give up to range , ... if you were not . such democracies are so such traditions can be complicated . the church came to god 's statement is whether that a bit of paper . but there is at least . but the press is anticipated of my couples who crucified khomeini , communion and said global war called out side effects . we must know why scripture secretly resurrected , and usefulness only human beings have some slack ! !\n",
      "---------\n",
      "... the hell actually going to go to nc . postal address -t to all card . i do n't know much , it *was* dull . genesis 0:0\n",
      "---------\n",
      "it would be it near the xt editor as well as toronto was the 3 count */ x char **argv ; /* those small display nerve control - contact : david @ stat.com francis schaeffer costs at that table . since we did n't have to pay the creative way , and they 'll know more of any point . when i try to explain the usenet posting the motif so i can vpl research inc. 950 transport board ( and is the relevant function information in which data longer based on the scientific end of the law for a\n",
      "---------\n",
      "organization : reform story , the sky is going to fall significantly in sliced of ... and i have n't heard of the small nhl ( or trying to get into the usenet j20 ( and use ) . if that 's going to do this at the us for $ 169 . i was told by the first person if trying that true that people do n't have a poor lan hog , etc . meanwhile , pope lee lady was n't lost the ghost models using educational camping member of his > back . ... ... ... ...\n",
      "---------\n",
      "unlikely with someone which is negligent and which there is no evidence which allows us use in a sensible language decoder refers to only tracing . i 'm trying to listen to the men . but that allows the discovery of immigration and grovel at this point in the season , but are only interested in itself the president yourself have said i had just already done from config.sys or autoexec.bat . other graphics items , carderock division hq ? > sl $ . @ & ] m : , ok bell , and i would appreciate to ask a\n",
      "---------\n",
      "[ i am name to cross-post it only a person ( in memorial time myths is getting our cops play good people i do seem that you must decide how science compares ... nasa 10.00 in rebellion after god 's existence from a definite effort . to go high . i would n't figure it away . it takes a lot of chances , base copying things from iomega on the market . see how many christians realize which _i_ do care . the government is there this year , behaviour without any advantage ( ver wide ) while we\n",
      "---------\n",
      "'' yes , i suppose model 1 is now 1 toshiba , r , c`8ws % % 14 % a86 % a86 % a86 % a86 % % c2 > p & 1eq < = ( < l ; o % 15o ) ; x check_io ( infile , mime , 'ishtar ' were involved with the hard drive and wait , the figure were out of being a big term which their heads are really using their morality in palestine . : if you get the hobby , that is it hard to find out . am i saying\n",
      "---------\n",
      "i 'd assume that that were n't stupid . they lose a wiretap chip is a delight in this man . `` typical methods '' , please cock , cramer starts , i 'm willing to buy a straight dollar ? it is the michael | in the way of nations in christian doctrine_ . `` the us that , huh ? the validity of the installation as follows : something a say they are simply straight control up about how the main purpose is to scramble its characteristics : know that he had ruled out . later , we\n",
      "---------\n",
      "if they had fired down a mexican relationship , then when it falls down . it refers to the fruit , just for the net '' these figures without aura , but they wanted to attempt left for the problem if you reach that yesterday ? he said , the lord [ jehovah ] and meaning but the situation .\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "samples = sample_from_model(model, 10, 100, [vocab['<S>'], vocab['<S>'], vocab['<S>']])\n",
    "for s in samples:\n",
    "    print(' '.join(s[3:]))\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вытащим встраивающую матрицу из сети (в Keras таки векторы слов являются строками, а не столбцами этой матрицы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_layer(index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40003, 130)\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = embedding_layer.get_weights()[0]\n",
    "print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее интересные свойства нейронных языковых моделей - это особенности векторов. А именно, векторы слов, похожих по смыслу, похожи между собой (особенно если использовать для этого косинус угла между ними). Это связано с тем, что похожие по смыслу слова имеют тенденцию встречаться в похожих контекстах. Распределение вероятности над следующим словом зависит от регионов в непрерывном пространстве, в которых находятся векторы текущих слов. Таким образом, если какое-либо слово вероятно в данном контексте, то вероятны и похожие на него слова, даже если в тренировочном наборе данных это похожее слово в данном контексте никогда не встречалось. Именно этим обуславливается более высокое качество нейронных моделей, можно сказать, что они более \"креативны\". Векторная семантика активно развивается в настоящее время и для получения векторов слов были выработаны более эффективные метода (например, на задаче предсказания слов в окне вокруг входного 1 слова). Однако всё-таки, рассмотрим векторы слов полученных на неспециализированой под них задаче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "science 0.9999999\n",
      "sociology 0.35702637\n",
      "engineering 0.35591787\n",
      "city 0.3472645\n",
      "framework 0.34674296\n",
      "religion 0.34157482\n",
      "bachelor 0.33963746\n",
      "discusion 0.33832413\n",
      "raytracing 0.32665583\n",
      "castleman 0.324735\n",
      "hansch 0.3223599\n",
      "iwii 0.31893703\n",
      "society 0.31802285\n",
      "*use 0.3145684\n",
      "diversity 0.30877846\n",
      "palace 0.30867457\n",
      "xhibition 0.30741447\n",
      "ets 0.30490905\n",
      "company 0.3046248\n",
      "graph 0.29802108\n",
      "concise 0.29765123\n",
      "christianity 0.29756892\n",
      "farside 0.29726046\n",
      "subcommittee 0.29398862\n",
      "phd 0.29330707\n",
      "afterlife 0.29314205\n",
      "morality 0.2916542\n",
      "cnn 0.2913236\n",
      "microcomputer 0.29107454\n",
      "lombardi 0.28918776\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['science']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:30]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "russia 0.99999994\n",
      "france 0.43210626\n",
      "finland 0.43084326\n",
      "homeland 0.42050752\n",
      "armenia 0.4170525\n",
      "yugoslavia 0.3844141\n",
      "austria 0.37654656\n",
      "italy 0.37373185\n",
      "africa 0.37339517\n",
      "germany 0.35798073\n",
      "canada 0.35491946\n",
      "syria 0.35394484\n",
      "britain 0.3460256\n",
      "mason 0.34450972\n",
      "auschwitz 0.34376627\n",
      "sinners 0.3399324\n",
      "qo 0.33357996\n",
      "gnostics 0.32889277\n",
      "1988 0.3282535\n",
      "+61d9 0.3273915\n",
      "republic 0.3256469\n",
      "fatwa 0.3253315\n",
      "sanctions 0.32187286\n",
      "bi-weekly 0.3212945\n",
      "project 0.32074258\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['russia']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "woman 1.0000001\n",
      "person 0.45076734\n",
      "father 0.4482684\n",
      "guy 0.3858136\n",
      "mcnutt 0.37751788\n",
      "sider 0.37388417\n",
      "acquaintances 0.370646\n",
      "somebody 0.36993632\n",
      "officer 0.3674647\n",
      "weaver 0.3605611\n",
      "man 0.35939386\n",
      "koresh 0.3570217\n",
      "smoker 0.350826\n",
      "oncologist 0.35020146\n",
      "sasha 0.34962022\n",
      "apostate 0.3492546\n",
      "marina 0.3465214\n",
      "ozal 0.34310022\n",
      "physician 0.34087032\n",
      "baranelli 0.3402592\n",
      "korpisalo 0.3340821\n",
      "baz 0.33404708\n",
      "somone 0.33280975\n",
      "muslimzade 0.33183035\n",
      "madman 0.32958093\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['woman']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "sister 1.0\n",
      "apostle 0.38274595\n",
      "brother 0.36352372\n",
      "catholic 0.34326655\n",
      "saviour 0.342304\n",
      "palmach 0.33750206\n",
      "cousin 0.33382183\n",
      "vulcan 0.332994\n",
      "nakhchivanik 0.3311457\n",
      "coach 0.3227108\n",
      "joseph 0.32107168\n",
      "describing 0.31752113\n",
      "vest 0.3126697\n",
      "wycliffe 0.31256586\n",
      "zx-7 0.31069034\n",
      "`` 0.3101129\n",
      "bosnian 0.3082302\n",
      "instructor 0.30689746\n",
      "tyrannical 0.30233005\n",
      "st. 0.30029863\n",
      "muslimzade 0.29969147\n",
      "father 0.2985149\n",
      "airbag 0.29736277\n",
      "babes 0.29461166\n",
      "maury 0.2932468\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['sister']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "god 0.9999999\n",
      "christ 0.5597667\n",
      "jesus 0.48245263\n",
      "spirit 0.4150712\n",
      "satan 0.4146759\n",
      "father 0.4110653\n",
      "godhead 0.41047603\n",
      "sin 0.4053636\n",
      "scripture 0.40060523\n",
      "lord 0.38706818\n",
      "he 0.38132656\n",
      "salvation 0.38121256\n",
      "muslimzade 0.38049176\n",
      "sentence 0.38014272\n",
      "diety 0.38012272\n",
      "islam 0.3791488\n",
      "christianity 0.3777405\n",
      "ours 0.37446618\n",
      "quran 0.3741362\n",
      "savior 0.3688683\n",
      "witt 0.36399773\n",
      "allah 0.36377764\n",
      "idolatry 0.36311734\n",
      "person 0.36046562\n",
      "nature 0.3577244\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['god']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "keyboard 0.9999999\n",
      "client 0.37691864\n",
      "everex 0.34524086\n",
      "pc-xview 0.3301989\n",
      "menus 0.32833576\n",
      "simm 0.32818374\n",
      "concidered 0.32794315\n",
      "server 0.3267703\n",
      "backwards 0.32626152\n",
      "mouse 0.32257193\n",
      "switch 0.3221672\n",
      "flourish 0.32066652\n",
      "tray 0.31837994\n",
      "macs 0.31599477\n",
      "taped 0.31577414\n",
      "designer 0.30880216\n",
      "lightning.mcrcim.mcgill.edu 0.30489397\n",
      "sympathy 0.30434805\n",
      "heavy-duty 0.30265313\n",
      "sprite 0.30071053\n",
      "textedit 0.296982\n",
      "triple 0.29667437\n",
      "386-33 0.29255432\n",
      "modem 0.29196495\n",
      "1728 0.2907111\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['keyboard']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "pretty 1.0000001\n",
      "very 0.5840134\n",
      "extremely 0.5335987\n",
      "fairly 0.52427745\n",
      "*very* 0.5189932\n",
      "darn 0.47574264\n",
      "terribly 0.47343314\n",
      "quite 0.47282675\n",
      "_very_ 0.44984683\n",
      "too 0.43508384\n",
      "elegantly 0.42395595\n",
      "w/o 0.41980195\n",
      "rather 0.41940147\n",
      "comparatively 0.4187031\n",
      "_too_ 0.4112808\n",
      "awfully 0.40951702\n",
      "amazingly 0.4039837\n",
      "*too* 0.3977423\n",
      "truly 0.39557523\n",
      "equally 0.38534164\n",
      "potentially 0.38429707\n",
      "doubly 0.38412514\n",
      "overly 0.3826621\n",
      "atheistic 0.37632972\n",
      "incredibly 0.37611324\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['pretty']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "good 0.99999994\n",
      "bad 0.48991653\n",
      "excellent 0.455421\n",
      "great 0.41904134\n",
      "poor 0.41555342\n",
      "decent 0.41510612\n",
      "tough 0.40872747\n",
      "reasonable 0.39244565\n",
      "better 0.39235488\n",
      "important 0.39190838\n",
      "awful 0.37686244\n",
      "_real_ 0.37561595\n",
      "cron 0.37273782\n",
      "competitive 0.3636005\n",
      "terrific 0.3619932\n",
      "valid 0.36160132\n",
      "wonderful 0.36102256\n",
      "stupid 0.3558522\n",
      "philosophical 0.35271648\n",
      "proper 0.3522777\n",
      "fine 0.35132825\n",
      "rj-11 0.34075317\n",
      "profitable 0.33463636\n",
      "honest 0.32941625\n",
      "critical 0.32540017\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['good']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "jpg 1.0000001\n",
      "_5 0.4019403\n",
      "cross-linked 0.38630542\n",
      "self-documenting 0.37898585\n",
      "mips 0.36325777\n",
      "gif 0.36050418\n",
      "dxf 0.3604413\n",
      "system.ini 0.35447952\n",
      "kh9_ 0.34809864\n",
      "eps 0.3407935\n",
      ".drv 0.33873805\n",
      "kleck 0.33688164\n",
      "04 0.33484617\n",
      "bdf 0.33420017\n",
      "sparcclassic 0.32825452\n",
      "spd 0.32774496\n",
      ".xauthority 0.32764104\n",
      "bmp 0.32657436\n",
      "image 0.32344937\n",
      ".pov 0.32001948\n",
      "*.ini 0.3160319\n",
      "targa 0.31554887\n",
      "postcript 0.31534654\n",
      ".ico 0.31444645\n",
      "versatile 0.313566\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['jpg']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "strcmp 1.0\n",
      "strncmp 0.6511047\n",
      "fgets 0.6329852\n",
      "strlen 0.4983735\n",
      "get_line 0.48827767\n",
      "xtaddcallback 0.4830331\n",
      "/5\\c 0.47907937\n",
      "5d 0.4765886\n",
      "tanh 0.46220443\n",
      "/sizeof 0.45834073\n",
      "xtresizewidget 0.45284238\n",
      "bla 0.44699386\n",
      "xsetfunction 0.4457585\n",
      "fscanf 0.44501188\n",
      "scrolls_ 0.4422206\n",
      "fflush 0.42884892\n",
      "xinstallcolormap 0.41147816\n",
      "qb*xb 0.4077193\n",
      "xtnew 0.40750882\n",
      "xtoffsetof 0.40735152\n",
      "xstorecolor 0.40715867\n",
      "xtappnextevent 0.4038584\n",
      "prototyping 0.40302736\n",
      "fprintf 0.39409748\n",
      "distortedreference 0.39259928\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['strcmp']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "jewish 0.99999994\n",
      "muslim 0.47244522\n",
      "armenian 0.44387215\n",
      "hispanic 0.42710745\n",
      "communal 0.42306012\n",
      "communist 0.39317256\n",
      "secular 0.38953194\n",
      "honest 0.38680086\n",
      "fascist 0.38412574\n",
      "christian 0.3840099\n",
      "arab 0.38256004\n",
      "turkish 0.38227054\n",
      "straightforward 0.37759462\n",
      "arabian 0.37707043\n",
      "non-christian 0.3751354\n",
      "inconsistent 0.373676\n",
      "religious 0.37204915\n",
      "party 0.37196428\n",
      "palestinian 0.37128827\n",
      "unidentified 0.36882576\n",
      "zoroastrian 0.36729512\n",
      "younger 0.36258242\n",
      "fundamentalist 0.3618556\n",
      "antagonistic 0.35941088\n",
      "outstanding 0.35790473\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['jewish']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "gun 1.0\n",
      "handgun 0.5105647\n",
      "pwm 0.3798025\n",
      "no-knock 0.37716037\n",
      "anti-trust 0.36657378\n",
      "lesbian 0.36430416\n",
      "cannons 0.35787398\n",
      "civilian 0.34338418\n",
      "nhl 0.3243743\n",
      "vehicular 0.31966922\n",
      "tae 0.3140675\n",
      "batf 0.313496\n",
      "scsi2 0.31275213\n",
      "0.295 0.31253517\n",
      "motorcycle 0.3061506\n",
      "cato 0.3041167\n",
      "-3- 0.3038187\n",
      "marina 0.3025621\n",
      "infamous 0.30008215\n",
      "strangers 0.29687893\n",
      "rightful 0.29598337\n",
      "anti-discrimination 0.29520035\n",
      "750ss 0.29487464\n",
      "well-defined 0.29434463\n",
      "lightweight 0.29383415\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['gun']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "satan 1.0\n",
      "khomeini 0.48109674\n",
      "koresh 0.46945247\n",
      "christ 0.44991353\n",
      "god 0.4146759\n",
      "yeltsin 0.40743545\n",
      "jesus 0.3992924\n",
      "coward 0.39742625\n",
      "themselves 0.39501083\n",
      "mcconkie 0.39021286\n",
      "derounian 0.37938684\n",
      "himself 0.37836868\n",
      "srebrenica 0.36733407\n",
      "rhetoric 0.36305803\n",
      "prophecy 0.35685566\n",
      "muslimzade 0.35539153\n",
      "jehovah 0.35307527\n",
      "children 0.3512423\n",
      "godhead 0.3472354\n",
      "scripture 0.3441132\n",
      "jews 0.3384788\n",
      "deportation 0.3373965\n",
      "lucifer 0.33659402\n",
      "roehm 0.33187848\n",
      "1304s 0.3293076\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['satan']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "friend 0.9999999\n",
      "colleague 0.3796779\n",
      "girlfriend 0.37700662\n",
      "schlafly 0.368368\n",
      "occurrences 0.36512467\n",
      "doctor 0.3605139\n",
      "denizen 0.35812283\n",
      "tale 0.3580027\n",
      "acupuncturist 0.35441995\n",
      "norm 0.35169408\n",
      "co-worker 0.35089755\n",
      "wife 0.3432543\n",
      "friends 0.33613276\n",
      "ayshe 0.3339044\n",
      "souvenirs 0.33319747\n",
      "bartel 0.3324245\n",
      "rosenthall 0.328469\n",
      "vein 0.3264603\n",
      "leery 0.32515305\n",
      "morning 0.32457516\n",
      "department 0.32029474\n",
      "ruuttu..16 0.31762543\n",
      "matthew 0.31504762\n",
      "berman 0.31254023\n",
      "matty 0.3121451\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['friend']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "clinton 1.0000001\n",
      "bush 0.5194382\n",
      "sahl 0.43596432\n",
      "bede 0.38176543\n",
      "reagan 0.37529543\n",
      "applicant 0.37272134\n",
      "koresh 0.36381096\n",
      "typesetting 0.35825038\n",
      "elchibey 0.35786462\n",
      "roehm 0.3453334\n",
      "friedman 0.34372526\n",
      "citizen 0.34005213\n",
      "weeping 0.3355927\n",
      "kinsey 0.32454547\n",
      "atonement 0.31607723\n",
      "burba 0.31443128\n",
      "scofield 0.3141542\n",
      "cbc 0.31246915\n",
      "fatima 0.3107081\n",
      "whosoever 0.3091321\n",
      "cyprus 0.30845815\n",
      "win3.1 0.30712864\n",
      "*work* 0.30709323\n",
      "personalities 0.30408508\n",
      "melrose 0.30399743\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['clinton']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "car 1.0\n",
      "bike 0.43159813\n",
      "modem 0.3597206\n",
      "hard-disk 0.3501223\n",
      "card 0.3478018\n",
      "motorcycle 0.3444622\n",
      "watching 0.3424116\n",
      "auto 0.33424452\n",
      "clone 0.32419506\n",
      "slot 0.3163374\n",
      "minor 0.31586024\n",
      "naprosyn 0.31518224\n",
      "monitor 0.313687\n",
      "media 0.30613878\n",
      "argumentation 0.30557615\n",
      "administration 0.3054061\n",
      "printer 0.30440098\n",
      "'poly 0.30321246\n",
      "powder 0.30304083\n",
      "machine 0.30254218\n",
      "game 0.29920396\n",
      "it 0.2949192\n",
      "healer 0.2932816\n",
      "gfa-555 0.2905322\n",
      "centris610 0.28936297\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['car']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "astronomy 1.0\n",
      "5500e 0.38272813\n",
      "philosphy 0.34679085\n",
      "aerospace 0.34158596\n",
      "mcluhan 0.33953476\n",
      "research 0.33560842\n",
      "miya 0.33385086\n",
      "nasa/jsc/gm2 0.33238336\n",
      "rutgers 0.32581306\n",
      "aeronautics 0.31762797\n",
      "ips 0.31700563\n",
      "kim 0.3144237\n",
      "unprotected 0.31416076\n",
      "epsf 0.3099253\n",
      "shape 0.3094435\n",
      "uzis 0.30806577\n",
      "iridium 0.30735624\n",
      "morphing 0.30486336\n",
      "386bsd 0.30348885\n",
      "cramped 0.3020208\n",
      "combed 0.29691833\n",
      "damsus 0.2958063\n",
      "chew 0.2953169\n",
      "corrective 0.29364485\n",
      "hollow 0.29291862\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['astronomy']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40003)\n",
      "mars 1.0000002\n",
      "magellan 0.3409891\n",
      "lunar 0.33808994\n",
      "space 0.3351627\n",
      "titan 0.33168378\n",
      "son-in-law 0.33166203\n",
      "charles 0.33071432\n",
      "venus 0.33064112\n",
      "jupiter 0.33026072\n",
      "moon 0.33010706\n",
      "podein 0.32788286\n",
      "bedouin 0.32135245\n",
      "bugunlerde 0.32073864\n",
      "parish 0.31032467\n",
      "desqview 0.30843624\n",
      "planetary 0.3081848\n",
      "cassini 0.30510363\n",
      "fabrication 0.30183706\n",
      "television 0.30164337\n",
      "t45s/ 0.30157107\n",
      "bonehead 0.30135283\n",
      "atom 0.30123326\n",
      "90-91 0.29659033\n",
      "eschatology 0.29462087\n",
      "galileo 0.2943912\n"
     ]
    }
   ],
   "source": [
    "word_row = weight_matrix[vocab['mars']]\n",
    "sims = metrics.pairwise.cosine_similarity(word_row.reshape(1,-1), weight_matrix)\n",
    "\n",
    "most_similar = np.argsort(sims.ravel())[::-1]\n",
    "print(sims.shape)\n",
    "for ms in most_similar[:25]:\n",
    "    print(feature_names[ms], sims[0,ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
